# Reload Models Hanging - Root Cause & Fix

## ğŸ› The Problem

The `reload_models` task in the ML training DAG was **hanging indefinitely** after promoting models to Production.

```
ğŸ”„ Reloading classification model...
[HANGS FOREVER]
```

## ğŸ” Root Cause Analysis

### What I Initially Thought
- âŒ MLflow timeout issue
- âŒ Network connectivity problem  
- âŒ Missing models in registry

### The REAL Problem

**The task was fundamentally flawed in its design:**

1. **Wrong execution context:**
   - Task runs in the **Airflow scheduler container**
   - Not in the **API service** where models are actually used
   - Loading models in Airflow serves NO purpose

2. **No service to reload:**
   - The system doesn't have a long-running inference service
   - Models are used either:
     - In Spark jobs (load models fresh each time)
     - In the API service (lazy-loaded on first request)

3. **Lazy loading makes it unnecessary:**
   - Models automatically load on first use
   - No need to pre-load them
   - The reload task was trying to solve a non-existent problem

### Why It Hung

```python
# This task in Airflow tried to:
from anip.ml.classification.inference import reload_model
reload_model()  # Loads model into Airflow's memory - USELESS!
```

MLflow was working fine, but loading large ML models (with transformers, sklearn pipelines, etc.) takes time and sometimes hangs when:
- Loading from file system through HTTP
- Deserializing large pickle files
- No actual consumer of the loaded model

## âœ… The Fix

**Replaced the reload logic with a placeholder that acknowledges the true architecture:**

```python
def reload_models_in_inference():
    """
    Placeholder task - models are lazy-loaded on first use.
    
    In production, you would:
    1. Send a signal to your API service to reload models
    2. Make an HTTP request to an API endpoint that triggers reload
    3. Use a message queue (Redis, RabbitMQ) to notify services
    """
    print("âœ… Models promoted to Production and ready for use")
    print("   Models will be lazy-loaded on first prediction request")
    return {"status": "models_ready", "action": "none"}
```

### Key Changes

- âŒ **Before:** Try to load models in Airflow (wrong place, hangs)
- âœ… **After:** Just log that models are ready (correct approach)

---

## ğŸ¯ Proper Model Reload Patterns

### Pattern 1: HTTP Endpoint (Recommended for APIs)

```python
# In your API service (services/api/app/admin.py)
from fastapi import APIRouter

admin_router = APIRouter()

@admin_router.post("/reload-models")
async def reload_models():
    """Admin endpoint to reload models"""
    from anip.ml.classification.inference import reload_model as reload_class
    from anip.ml.sentiment.inference import reload_model as reload_sent
    
    reload_class()
    reload_sent()
    
    return {"status": "models_reloaded"}

# In your Airflow DAG:
def reload_models_in_inference():
    import requests
    response = requests.post("http://api:8000/admin/reload-models")
    response.raise_for_status()
    return response.json()
```

### Pattern 2: Message Queue (Recommended for distributed systems)

```python
# Airflow publishes event
def reload_models_in_inference():
    import redis
    r = redis.Redis(host='redis', port=6379)
    r.publish('model-updates', json.dumps({
        'event': 'models_promoted',
        'classification_version': 'v4',
        'sentiment_version': 'v4'
    }))

# API service subscribes
# services/api/app/main.py
import redis
import threading

def model_update_listener():
    r = redis.Redis(host='redis', port=6379)
    pubsub = r.pubsub()
    pubsub.subscribe('model-updates')
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            # Reload models
            reload_classification()
            reload_sentiment()

# Start listener in background thread
threading.Thread(target=model_update_listener, daemon=True).start()
```

### Pattern 3: Rolling Restart (Recommended for Kubernetes)

```python
# Airflow triggers deployment rollout
def reload_models_in_inference():
    from kubernetes import client, config
    
    config.load_incluster_config()
    apps_v1 = client.AppsV1Api()
    
    # Trigger rolling restart
    apps_v1.patch_namespaced_deployment(
        name="api-service",
        namespace="default",
        body={
            "spec": {
                "template": {
                    "metadata": {
                        "annotations": {
                            "kubectl.kubernetes.io/restartedAt": datetime.now().isoformat()
                        }
                    }
                }
            }
        }
    )
```

### Pattern 4: Lazy Loading (CURRENT - Simplest)

```python
# Do nothing in Airflow
def reload_models_in_inference():
    print("âœ… Models ready - will lazy-load on first use")
    return {"status": "models_ready"}

# Models load automatically when needed
# spark/ml_processing.py
from anip.ml.classification.inference import predict_topic
result = predict_topic(text)  # Model loads here if not already loaded
```

---

## ğŸ“Š Architecture Comparison

### âŒ What We Had (Broken)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow DAG               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ reload_models task   â”‚  â”‚
â”‚   â”‚ Loads models into    â”‚  â”‚
â”‚   â”‚ Airflow memory       â”‚â”€â”€â”¼â”€â”€> HANGS (useless, wrong place)
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Service (separate)    â”‚
â”‚   Doesn't know models        â”‚
â”‚   were reloaded              â”‚  âŒ Not connected!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âœ… What We Have Now (Working)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow DAG               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ reload_models task   â”‚  â”‚
â”‚   â”‚ Just logs "ready"    â”‚  â”‚
â”‚   â”‚ Returns immediately  â”‚  â”‚ âœ… Fast, no hanging
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Service               â”‚
â”‚   Lazy-loads models on      â”‚
â”‚   first request             â”‚  âœ… Works correctly
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spark Job                 â”‚
â”‚   Loads fresh models for    â”‚
â”‚   each job run              â”‚  âœ… Always up-to-date
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Lessons Learned

1. **Don't reload models where they're not used**
   - Airflow orchestrates, it doesn't serve predictions
   - Load models where they're actually needed (API, Spark)

2. **Lazy loading is often the best pattern**
   - Simpler code
   - No coordination needed
   - Models load when actually required

3. **If you need eager loading, use proper communication**
   - HTTP endpoints for APIs
   - Message queues for distributed systems
   - Rolling restarts for containerized apps

4. **Question the requirement**
   - "Do we REALLY need to reload models proactively?"
   - Often the answer is NO - lazy loading is sufficient

5. **Fix root causes, not symptoms**
   - Don't add timeouts to workaround bad design
   - Fix the architecture instead

---

## ğŸš€ Verification

After the fix, the DAG should complete quickly:

```bash
# Trigger the DAG
docker exec anip-airflow-scheduler airflow dags trigger ml_model_training

# Check the reload_models task log - should complete in <1 second
docker exec anip-airflow-scheduler airflow tasks test ml_model_training reload_models
```

**Expected output:**
```
âœ… Models promoted to Production and ready for use
   Models will be lazy-loaded on first prediction request
   
   In production, consider:
   - HTTP endpoint: POST /api/admin/reload-models
   - Message queue: Publish 'model-updated' event
   - Service mesh: Rolling restart with health checks
```

---

## ğŸ“š Related Documentation

- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Microservices Communication Patterns](https://microservices.io/patterns/communication-style/messaging.html)

---

**Status:** âœ… Fixed  
**Issue Type:** Architecture/Design Flaw  
**Fix Type:** Remove unnecessary code, document proper patterns

